# =============================================================================
# vLLM Benchmark Recipe - Custom Model Test
# =============================================================================
# Testing with facebook/opt-350m (larger than opt-125m)
# =============================================================================

configuration:
  target: "meluxina"

# =============================================================================
# Service: vLLM Inference Server
# =============================================================================
service:
  type: "vllm"
  name: "vllm-opt350"
  partition: "gpu"
  num_gpus: 1
  time_limit: "02:00:00"
  account: "p200981"
  
  # Using new advanced config features
  memory: "64G"                   # Request explicit memory
  
  settings:
    model: "facebook/opt-350m"    # Slightly larger model
    tensor_parallel_size: 1

# =============================================================================
# Client: Smoke Test
# =============================================================================
client:
  type: "vllm_smoke"
  partition: "cpu"
  num_gpus: 0
  time_limit: "00:30:00"
  
  settings:
    model: "facebook/opt-350m"
    prompt: "Explain the theory of relativity in simple terms."
    max_tokens: 100               # Generate more tokens
    warmup_delay: 45              # Extra time for larger model

# =============================================================================
# Benchmark Configuration
# =============================================================================
benchmarks:
  num_clients: 1
  metrics: ["latency", "throughput"]
