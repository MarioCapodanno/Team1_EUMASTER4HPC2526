configuration:
  target: "meluxina"

service:
  name: "vllm-llama"
  image: "vllm/vllm-openai:latest"
  command: "vllm serve --model meta-llama/Llama-2-7b-hf"
  partition: "gpu"
  num_gpus: 2
  time_limit: "04:00:00"
  port: 8000

client:
  command: "python3 -c 'import requests; import time; import os; url=os.environ.get(\"SERVICE_URL\"); start=time.time(); r=requests.post(f\"{url}/v1/completions\", json={\"model\":\"meta-llama/Llama-2-7b-hf\",\"prompt\":\"Hello\",\"max_tokens\":50}); print(f\"Latency: {time.time()-start:.2f}s\")'"
  partition: "cpu"
  num_gpus: 0
  time_limit: "00:30:00"

benchmarks:
  num_clients: 5
  metrics: ["latency", "throughput", "tokens_per_second"]
