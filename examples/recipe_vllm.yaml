# =============================================================================
# vLLM Benchmark Recipe (Simplified)
# =============================================================================
# LLM inference benchmark using vLLM OpenAI-compatible API.
# =============================================================================

configuration:
  target: "meluxina"

# =============================================================================
# Service: vLLM Inference Server
# =============================================================================
service:
  type: "vllm"
  name: "vllm-llama"
  partition: "gpu"
  num_gpus: 1
  time_limit: "04:00:00"
  
  settings:
    model: "facebook/opt-125m"  # Model to serve (use public models)
    tensor_parallel_size: 1     # Number of GPUs for tensor parallelism

# =============================================================================
# Client: Smoke Test
# =============================================================================
client:
  type: "vllm_smoke"
  partition: "cpu"
  num_gpus: 0
  time_limit: "00:30:00"
  
  settings:
    model: "facebook/opt-125m"
    prompt: "Hello, how are you?"
    max_tokens: 50
    warmup_delay: 30            # vLLM needs time to load model

# =============================================================================
# Benchmark Configuration
# =============================================================================
benchmarks:
  num_clients: 1
  metrics: ["latency"]
