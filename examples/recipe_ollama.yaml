# =============================================================================
# Ollama Benchmark Recipe (Simplified)
# =============================================================================
# Local LLM inference using Ollama.
# =============================================================================

configuration:
  target: "meluxina"

# =============================================================================
# Service: Ollama LLM Server
# =============================================================================
service:
  type: "ollama"
  name: "ollama-service"
  partition: "gpu"
  num_gpus: 1
  time_limit: "02:00:00"
  account: "p200981"
  
  settings:
    model: "llama2"             # Model to pull and serve
    warmup_seconds: 5           # Wait for server to start

# =============================================================================
# Client: Smoke Test
# =============================================================================
client:
  type: "ollama_smoke"
  partition: "cpu"
  num_gpus: 0
  time_limit: "00:10:00"
  
  settings:
    model: "llama2"
    num_requests: 5             # Number of inference requests
    max_retries: 30             # Max retries waiting for service

# =============================================================================
# Benchmark Configuration
# =============================================================================
benchmarks:
  num_clients: 2
  metrics: ["latency", "throughput"]
