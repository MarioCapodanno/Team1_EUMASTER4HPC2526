# =============================================================================
# vLLM Stress Test Recipe (Simplified)
# =============================================================================
# High-load LLM inference benchmark with multiple requests.
# =============================================================================

configuration:
  target: "meluxina"

# =============================================================================
# Service: vLLM Inference Server
# =============================================================================
service:
  type: "vllm"
  name: "vllm-stress"
  partition: "gpu"
  num_gpus: 1
  time_limit: "04:00:00"
  
  settings:
    model: "facebook/opt-125m"
    tensor_parallel_size: 1

# =============================================================================
# Client: Stress Test
# =============================================================================
client:
  type: "vllm_stress"
  partition: "cpu"
  num_gpus: 0
  time_limit: "01:00:00"
  
  settings:
    model: "facebook/opt-125m"
    num_requests: 50            # Requests per client
    max_tokens: 64              # Max tokens per response
    warmup_delay: 30            # Wait for model to load

# =============================================================================
# Benchmark Configuration
# =============================================================================
benchmarks:
  num_clients: 4
  metrics: ["avg_latency", "p50_latency", "p95_latency", "throughput"]
