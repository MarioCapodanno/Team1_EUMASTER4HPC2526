# Artifact Collection and Reporting Guide

## Overview

The AI Factory Benchmarking Framework features a **fully automatic** artifact-first pipeline. Simply run a benchmark and everything happens automatically:

‚ú® **Ultra User-Friendly Workflow:**
```bash
python src/frontend.py examples/recipe_postgres_stress.yaml
```

That's it! The framework will:
1. Deploy your service and clients
2. Wait for benchmark completion
3. Automatically collect artifacts from the cluster
4. Generate a complete report with plots
5. Show you where to find your results

## Automatic Workflow

### What Happens Automatically

When you run a benchmark:

1. **Deployment** - Service and clients deployed to Meluxina
2. **Execution** - Clients emit per-request JSONL metrics
3. **Monitoring** - Framework polls for completion (max 1 hour)
4. **Collection** - JSONL files downloaded from cluster automatically
5. **Aggregation** - Metrics computed into summary.json
6. **Reporting** - Markdown report and plots generated
7. **Done!** - Results ready to view

### Example Output

```bash
$ python src/frontend.py examples/recipe_postgres_stress.yaml

============================================================
Benchmark ID: 27
============================================================
[... deployment messages ...]

============================================================
Waiting for benchmark to complete...
============================================================

  [23:25:30] Progress: 4/4 clients completed

‚úì All clients completed!

============================================================
Collecting artifacts from cluster...
============================================================

Found 4 JSONL file(s)
  Downloading requests_client-1.jsonl...
    ‚úì Downloaded
  [... more files ...]
‚úì Merged into requests.jsonl (15000 lines)

‚úì Artifacts collected successfully!

============================================================
Generating benchmark report...
============================================================

  Loading request data...
  Aggregating metrics...
  Generating report files...
  Generating plots...

‚úì Report generated!

üìä View your results:
  - Report: reports/27/report.md
  - Plots: reports/27/plots/
```

## Manual Control (Advanced)

For advanced users who want manual control:

### Skip Automatic Collection

Press `Ctrl+C` during the "Waiting for benchmark to complete" phase to skip automatic collection. You can then manually collect later:

```bash
# Collect artifacts manually
python src/frontend.py --collect <benchmark_id>

# Generate report manually
python src/frontend.py --report <benchmark_id>
```

### Compare Benchmarks

```bash
python src/frontend.py --compare <baseline_id> <current_id>
```

## Artifact Structure

```
results/<benchmark_id>/
‚îú‚îÄ‚îÄ run.json          # Benchmark metadata (recipe, jobs, timestamps)
‚îú‚îÄ‚îÄ requests.jsonl    # Per-request data (one JSON per line)
‚îî‚îÄ‚îÄ summary.json      # Aggregated metrics (generated by --report)

reports/<benchmark_id>/
‚îú‚îÄ‚îÄ report.md         # Human-readable Markdown report
‚îú‚îÄ‚îÄ report.json       # Structured JSON report
‚îî‚îÄ‚îÄ plots/            # PNG visualizations
    ‚îú‚îÄ‚îÄ latency_percentiles.png
    ‚îú‚îÄ‚îÄ throughput.png
    ‚îú‚îÄ‚îÄ error_distribution.png
    ‚îú‚îÄ‚îÄ latency_histogram.png
    ‚îî‚îÄ‚îÄ service_metrics.png
```

## JSONL Format

Each client emits one JSON object per line to `requests.jsonl`:

### Metadata Line (first line)
```json
{"benchmark_id": "27", "service_type": "postgres", "test_start": "2026-01-07T23:21:00Z"}
```

### Request Lines
```json
{"timestamp_start": 1704624000, "timestamp_end": 1704624001, "latency_s": 0.01, "success": true, "service_type": "postgres", "request_id": "insert_1", "operation_type": "insert", "rows_affected": 1, "query_type": "point_insert"}
```

### Required Fields
- `timestamp_start`: Unix timestamp (seconds)
- `timestamp_end`: Unix timestamp (seconds)
- `latency_s`: Latency in seconds (float)
- `success`: Boolean
- `service_type`: "vllm", "ollama", "postgres", etc.
- `request_id`: Unique identifier for this request

### Service-Specific Fields

**LLM Services (vLLM, Ollama):**
- `output_tokens`: Number of generated tokens
- `input_tokens`: Number of prompt tokens
- `model`: Model name
- `prompt`: Input prompt (optional)

**Database Services (PostgreSQL):**
- `operation_type`: "insert", "select", "update", "delete"
- `query_type`: "point_lookup", "range_scan", "aggregation", etc.
- `rows_affected`: Number of rows (optional)

## Supported Clients with JSONL Output

‚úÖ **Fully Implemented:**
- `postgres_stress` - PostgreSQL stress test with inserts/selects
- `postgres_smoke` - PostgreSQL connectivity test
- `vllm_stress` - vLLM inference stress test
- `vllm_smoke` - vLLM smoke test
- `ollama_smoke` - Ollama smoke test

‚ö†Ô∏è **Needs Implementation:**
- `chroma_stress` - ChromaDB stress test
- `chroma_healthcheck` - ChromaDB health check

## Example Complete Workflow

```bash
# 1. Run benchmark
python src/frontend.py examples/recipe_postgres_stress.yaml
# Output: Benchmark ID: 27

# 2. Wait for completion (or use --watch)
python src/frontend.py --watch 27

# 3. Collect artifacts
python src/frontend.py --collect 27

# 4. Generate report
python src/frontend.py --report 27

# 5. View report
cat reports/27/report.md
```

## Troubleshooting

### Benchmark takes too long
- The framework waits up to 1 hour for completion
- If timeout occurs, you can manually collect later:
  ```bash
  python src/frontend.py --collect <benchmark_id>
  python src/frontend.py --report <benchmark_id>
  ```

### Want to skip automatic collection
- Press `Ctrl+C` during the waiting phase
- Benchmark will continue running on cluster
- Collect manually when ready

### Missing plots
- Install matplotlib: `pip install matplotlib`
- Plots are optional; reports will still generate without them

## Dependencies

Required:
- `numpy` - For percentile calculations
- `gitpython` - For git commit tracking

Optional:
- `matplotlib` - For plot generation

Install all:
```bash
pip install -r requirements.txt
```
